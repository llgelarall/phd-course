{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KUB5gpswan4"
      },
      "source": [
        "# Project 4: Classification task in Speck\n",
        "\n",
        "Similar to last session's task, you are called to develop a network usings sinabs simulator. You need to train the network using your aquired knowledge from past sessions. Your is to train the network to identify 5 different classes (ship, car, flower, dog, appple).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg0DBBiLfiYT"
      },
      "source": [
        "You should start by importing the libraries for your code.\n",
        "\n",
        "Below you will find the functions to visualize your input signal. You are encouraged to re-use functions that you may have developed in previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t23uqv7u0Mg9"
      },
      "outputs": [],
      "source": [
        "def animate_frames(frames, figure=None, interval: int = 20, **kwargs):\n",
        "    if figure is None:\n",
        "        figure, _ = plt.subplots(**kwargs)\n",
        "    ax = figure.gca()\n",
        "\n",
        "    image = ax.imshow(frames[0])  # .T)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    def animate(index):\n",
        "        image.set_data(frames[index])  # .T)\n",
        "        return image\n",
        "\n",
        "    anim = FuncAnimation(figure, animate, frames=len(frames), interval=interval)\n",
        "    video = anim.to_html5_video()\n",
        "    html = HTML(video)\n",
        "    display(html)\n",
        "    plt.tight_layout()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def events_to_frames(frames, polarity: bool = True):\n",
        "    if len(frames.shape) == 3:\n",
        "        frames = frames.unsqueeze(-1).repeat(1, 1, 1, 3)\n",
        "    else:\n",
        "        if not polarity:\n",
        "            frames = frames.abs().sum(-1)\n",
        "        elif polarity:\n",
        "            frames = torch.concat([frames, torch.zeros(frames.shape[0], 1, *frames.shape[2:], device=frames.device)], dim=1).movedim(1, -1)\n",
        "    frames = ((frames / frames.max()) * 255).int().clip(0, 255)\n",
        "    return frames\n",
        "\n",
        "# Visualize your input data\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qdvwnl-0Q_K"
      },
      "source": [
        "### Train a spiking neural network\n",
        "\n",
        "In this project you will develop a Spiking Convolutional Neural Network to solve a simple classification problem. You are encouraged to use Sinabs, to be able to load your network to the Speck chip.\n",
        "\n",
        "The data consist of Speck recordings where objects of each class have been recorded using the Dynamic Vision Sensor of speck2f.\n",
        "\n",
        "In this task, the data consist of 5 classes (apple, car, flower, dog, ship). To reduce time consumption, the data are already given in tensor format. Each calss's data are stored in the file `{class_name}_tensor.pt` (for example car_tensor.pt). Please note that the data are **not** in [sparse](https://pytorch.org/docs/stable/sparse.html) format. However, they do follow the address event representation framework.\n",
        "\n",
        "<b> Due to computational power and memory limitations you are advised to start your exploration with 2 or 3 classes and only a subset of each recording </b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwZSAvZ70hSe"
      },
      "outputs": [],
      "source": [
        "#### Task 3.0: Load the data\n",
        "\n",
        "apple_file, _ = urlretrieve(\"https://github.com/ncskth/phd-course/raw/main/book/module4/apple_tensor.pt\")\n",
        "apple_events = torch.load(apple_file)\n",
        "\n",
        "car_file, _ = urlretrieve(\"https://github.com/ncskth/phd-course/raw/main/book/module4/car_tensor.pt\")\n",
        "car_events = torch.load(car_file)\n",
        "\n",
        "apple = events_to_channels(apple_events)\n",
        "car = events_to_channels(car_events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXakjyQcvkuc"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Using the functions provided in previous tasks, visualize your input data. You can start by visualizing the first 10 seconds of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q905fbuRiW4U"
      },
      "outputs": [],
      "source": [
        "# Visualize your data\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v1SiVHawSD8"
      },
      "source": [
        "### Create the dataloader\n",
        "\n",
        "To be able to load the data to train the network, you should create a dataloader from the given data. In the cell bellow, you have to fill in the missing lines to create your dataloader\n",
        "\n",
        "You might find useful the [`random_split`](https://pytorch.org/docs/stable/data.html) function and the [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) module.\n",
        "\n",
        "To create the dataset, we split the recording into pieces of `sample_duration` duration, so called sample. Each of these samples are one input of the network, for which the network will have to identify what is shown in the input signal.\n",
        "\n",
        "We create the labels by creating tensors of zeros and ones (for the two classes) of size equal to the number of samples of each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycuQnRq9whkg"
      },
      "outputs": [],
      "source": [
        "class SpeckDataset(Dataset):\n",
        "    def __init__(self, frames, targets, transform=None, target_transform=None):\n",
        "        self.targets = targets\n",
        "        self.frames = frames\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frames = self.frames[idx]\n",
        "        label = self.targets[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return frames, label\n",
        "\n",
        "\n",
        "sample_duration = 1   # change this to suit your experiment. Higher value will result in longer sequence (video) for each input data point (sample) but less samples overall\n",
        "car_samples = car.shape[0]//sample_duration\n",
        "apple_samples = apple.shape[0]//sample_duration\n",
        "\n",
        "train_perc = 0.8\n",
        "batch_size = 10\n",
        "\n",
        "c = car[:sample_duration*car_samples].reshape((car_samples, sample_duration, *car.shape[1:]))\n",
        "a = apple[:sample_duration*apple_samples].reshape((apple_samples, sample_duration, *apple.shape[1:]))\n",
        "\n",
        "c_t = torch.zeros(c.shape[0])\n",
        "a_t = torch.zeros(a.shape[0])+1\n",
        "\n",
        "data = torch.cat((c, a), dim=0)\n",
        "targets = torch.cat((c_t, a_t), dim=0)\n",
        "\n",
        "\n",
        "# Create the dataset using the SpeckDataset module\n",
        "# dataset = ...\n",
        "\n",
        "# Split the data to trainset and testset\n",
        "# trainset, testset =\n",
        "\n",
        "# Define the trainloader and testloader DataLoaders\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636gk89PwIyf"
      },
      "source": [
        "### Develop your network\n",
        "\n",
        "Use the torch sequential and the `sl.IAFSqueeze()` modules to develop your network. Be aware os the [speck's architecture](https://synsense-sys-int.gitlab.io/samna/models/speckSeries/summary.html) regarding the size of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G36OC6LzRkT"
      },
      "outputs": [],
      "source": [
        "snn_bptt = nn.Sequential(\n",
        "    # ...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ1z6S0wzbpi"
      },
      "source": [
        "### Train your network\n",
        "\n",
        "You should implement your training and testing loop. Fill in the train and test function and evaluate the result.\n",
        "\n",
        "You are advised to use Google Colab's GPU resources to accelerate the training.\n",
        "\n",
        "Note that the `sl.IAFSqueeze()` module has the peculiarity that it cannot process data in the form of `[Batch, Time, Channel, Height, Width]`, but you first have to reshape your input data to the form `[Batch x Time, Channel, Height, Width]`. This is equivalent to stacking one sample after the other. Be aware that the output will have the same form as the input tensor, so you will have to reshape the output back to `[Batch, Time, Channel, Height, Width]`.\n",
        "\n",
        "Your output labels are not in the form of your target labels! Your decision on how you handle your output sequences to retrieve your output labels can affect the performance of your network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZJdNIPczacX"
      },
      "outputs": [],
      "source": [
        "# define the functions' signatures (parameters and return)\n",
        "def train():\n",
        "  for data, label in tqdm.tqdm(trainloader):\n",
        "    # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
        "    data = data.reshape(-1, 2, 128, 128)\n",
        "\n",
        "    # fill in the rest of the function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "  with torch.no_grad():\n",
        "    for data, label in tqdm.tqdm(testloader):\n",
        "      # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
        "      data = data.reshape(-1, 2, 128, 128)\n",
        "\n",
        "      # fill in the rest of the function\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "# optimizer = ...\n",
        "# criterion = ...\n",
        "\n",
        "# Train your network"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}